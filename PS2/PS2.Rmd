---
title: "assignment2"
author: "Shuai Yan, Ethan Tse"
date: "2023-10-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)

# set seed
seed_value <- 19710822
set.seed(seed_value)
```

# Define functions:

```{r shades of grey function}
phase2_spouse_idx <- function(use_rejection=False,
                              phase1_size,
                              score,
                              cutoff_score) {
  population <- length(score)
  if(use_rejection == TRUE) { 
    # question 3 inversely proportional rejection rate 
    
    # default last in population
    spouse_index <- population
    
    # get indices of all people who pass cutoff
    candidate_spouse_index <- phase1_size +
      which(score[(phase1_size+1):population] > cutoff_score)
    
    # get scores of all people who pass cutoff
    candidate_scores <- score[candidate_spouse_index]
    
    # guard against no candidates with scores > cutoff
    if(length(candidate_scores) > 0){
      # for each candidate, compute rejection rate,
      # and run a Bernoulli trial for reject or accept
      for(idx in length(candidate_scores)){
        candidate_score <- candidate_scores[idx]
        
        # the rejection probability is inversely proportion to like
        rejection_prob <- 1/candidate_score
        # `rbernoulli` gives FALSE when probability negative
        if(rejection_prob > 0){
          reject_or_accept <- !rbernoulli(n = 1, p = rejection_prob)
        } else {
          reject_or_accept <- FALSE
        }
        
        # upon first accept, break out of the loop
        if(reject_or_accept == 1){
          spouse_index <- idx
          break()
        }
      }
    }
    
  } else {
    # now select as your life partner the next date with a better score
    spouse_index <- phase1_size +
      which(score[(phase1_size+1):population] > cutoff_score)[1]
    if (is.na(spouse_index)) {
      spouse_index <- population
    }
  }
  
  return(spouse_index)
}

simulate_dating_2 <- function(population = 1000,
                              phase1_fraction = 1 / exp(1),
                              num_iterations = 1, 
                              distribution = data.frame(name="rnorm", sd=1),
                              use_rejection = FALSE) {
  spouse_scores <- numeric(length = num_iterations)
  phase1_size <- round(population * phase1_fraction)
  
  for (case_idx in 1:num_iterations) {
    # DATA GENERATION
    if(!distribution$name %in% c("rnorm", "runif")){
      stop("Error: Unspecified distribution `r distribution`")
    }
    # add options for sampling
    if(distribution$name == "rnorm"){
      stdev = distribution$sd
      score <- rnorm(population, sd = stdev)
    } else if(distribution$name == "runif"){
      score <- runif(population)
    } 
    
    # PHASE 1
    cutoff_score <- max(score[1:phase1_size])
    
    # PHASE 2
    spouse_index <- phase2_spouse_idx(use_rejection,
                                      phase1_size,
                                      score,
                                      cutoff_score)
    spouse_scores[case_idx] <- score[spouse_index]
  }
  
  return(spouse_scores)
}


score_and_plot_figure <- function(scores){
  scores_df <- as.data.frame(scores) 
  colnames(scores_df) <- c("score_0.15", "score_0.37")
  
  scores_df_long <- scores_df %>% 
    pivot_longer(cols = everything(), names_to = "phase1_fractions", values_to = "score")
  
  scores_histogram <- scores_df_long %>% 
    ggplot(aes(x = score)) + 
    geom_histogram(colour = "black", fill = "lightblue") + 
    theme_classic() + 
    facet_wrap(~phase1_fractions)
  
  return(scores_histogram)
}

```




## Part I

__Ans:__ 

+ The bimodal distributions have a peak at 0 because that is the case where the optimal score in the population is included in the phase 1 fraction, and in this situation, the spouse is the last person in the population. Therefore, the distribution of the first mode is the same as the distribution that the population was sampled from (i.e. standard normal distribution).

+ Two phase 1 fractions do not affect the score of main peaks of the histograms, but it affects the size of the first peak. When phase 1 ratio is lager then optimum ratio, the chance that the optimal spouse fall in phase is increased. We would prefer the shades-of-gray model as it shows more information from the spouse score distribution.


```{r question 1}
# Test fractions
phase1_fractions <- c(0.15, 0.37) #seq(0.05, 0.95, 0.02)
scores <- list() # list to save scores 
for (idx in 1:length(phase1_fractions)) {
  scores[[idx]] <- simulate_dating_2(population = 1000,
                                     phase1_fraction = phase1_fractions[idx],
                                     num_iterations = 10000)
}


q1_plot <- score_and_plot_figure(scores)
q1_plot
```

## Part II

__Ans:__

The optimal stopping point in terms of phase 1 fraction is basically the same when drawing from normal distributions with mean = 0 but varying standard deviation. What does change is the mean score (higher SD ~ higher mean score on the y axis). 

```{r question 2, fig.width=12, fig.heigth = 12}
# Test fractions
phase1_fractions <- seq(0.05, 0.95, 0.02)
stdev_fractions <- seq(1, 51, 5)

q2_sd_experiment <- list()
q2_scores <- list() # list to save scores 
for(sd_idx in 1:length(stdev_fractions)){
  for (idx in 1:length(phase1_fractions)) {
    
    iteration_mean <- mean(simulate_dating_2(population = 1000,
                                             phase1_fraction = phase1_fractions[idx],
                                             num_iterations = 10000, 
                                             distribution = data.frame(name="rnorm", sd=stdev_fractions[sd_idx])))
    
    q2_scores[[idx]] <- data.frame(mean = iteration_mean, phase1_fractions = phase1_fractions[idx], sd = stdev_fractions[sd_idx])
  }
  
  q2_scores_df <- bind_rows(q2_scores)
  q2_sd_experiment[[sd_idx]] <- q2_scores_df
}

multi_sd_df <- bind_rows(q2_sd_experiment)

max_scores_by_sd <- multi_sd_df %>% 
  group_by(sd) %>% 
  slice_max(mean)

q2_score_plot <- multi_sd_df %>% 
  ggplot(aes(x = phase1_fractions, y = mean, fill = factor(sd))) + 
  geom_point(colour = "black", shape = 21, size = 4) + 
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 90), 
        plot.title = element_text(hjust = 0.5, size = 15)) + 
  geom_vline(aes(xintercept = phase1_fractions), data=max_scores_by_sd, colour = "red", linetype = "dashed") +
  labs(x = "Phase 1 Fractions", 
       y = "Score") + 
  scale_x_continuous(n.breaks = 5) + 
  facet_wrap(.~factor(sd), scales = "free")

q2_score_plot
max_scores_by_sd

```

__Ans:__

The optimal stopping fraction in terms of phase 1 fraction is 0.04 = 4%. This makes sense because for a uniform distribution, we have a lot less information we need to learn (roughly min/max) because then within every score is equally probable. If the population is drawn from a uniform distribution, then all scores as equally likely. Thus, we can also see after a certain point, the score decreases linearly as we increase our chance of missing the person with the highest score as we do more sampling. 

```{r question 2, uniform distribution}
phase1_fractions <- seq(0, 0.95, 0.02)
q2_unif_scores <- list()
for (idx in 1:length(phase1_fractions)) {
  
  iteration_mean <- mean(simulate_dating_2(population = 1000,
                                           phase1_fraction = phase1_fractions[idx],
                                           num_iterations = 10000, 
                                           distribution = data.frame(name="runif")))
  
  q2_unif_scores[[idx]] <- data.frame(mean = iteration_mean, phase1_fractions = phase1_fractions[idx])
}

q2_scores_unif_df <- bind_rows(q2_unif_scores)

max_scores_by_unif_fraction <- q2_scores_unif_df %>% 
  slice_max(mean)

q2_scores_unif_df %>% 
  ggplot(aes(x = phase1_fractions, y = mean)) + 
  geom_point(colour = "black", fill = "lightblue", size = 4, shape = 21) +
  geom_vline(aes(xintercept = phase1_fractions), data=max_scores_by_unif_fraction, colour = "red", linetype = "dashed") +
  labs(title = "Scores drawn from a uniform distribution", 
       x = "Phase 1 Fractions", 
       y = "Score") + 
  scale_x_continuous(n.breaks = 20) + 
  theme_classic()
```

## Part III

Please see the function for details. 

We model this by computing the probability of rejection as 1/score (inversely proportional). This is then used to do a single bernoulli trial. 1 = accept. 0 = reject. 

When taking rejection into account, we would agree that the inversely proportional rejection rate is reasonable. When a dating candidate has a negative score, the person will always reject a propose. In this setup, the expectation score of a spouse is always close to 0, which indicates that applying the optimal stopping model here can not help getting a spouse with a higher score.

```{r question 3}

q3_scores <- list()
for (idx in 1:length(phase1_fractions)) {
iteration_mean <- mean(simulate_dating_2(population = 1000,
phase1_fraction = phase1_fractions[idx],
num_iterations = 10000, 
distribution = data.frame(name="rnorm", sd=1), 
use_rejection = TRUE))

q3_scores[[idx]] <- data.frame(mean = iteration_mean, phase1_fractions = phase1_fractions[idx])
}

q3_df <- bind_rows(q3_scores)

q3_df %>% ggplot(aes(x = phase1_fractions, y = mean)) + 
geom_point(colour = "black", fill = "lightblue", size = 4, shape = 21) +
labs(x = "Phase 1 Fractions", 
y = "Score") + 
ylim(-2, 2) + 
theme_classic()
```